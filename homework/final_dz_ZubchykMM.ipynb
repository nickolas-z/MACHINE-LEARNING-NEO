{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [–§—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ—î–∫—Ç](https://www.edu.goit.global/uk/learn/25315460/23598278/26258114/homework)\n",
    "___\n",
    "\n",
    "## Kaggle: [ML: Fundamentals and Applications 2025-03](https://www.kaggle.com/competitions/ml-fundamentals-and-applications-2025-03/leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –Ü–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö –º–æ–¥—É–ª—ñ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from typing import Sequence, Union, Optional\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, \n",
    "    OneHotEncoder, \n",
    "    PowerTransformer, \n",
    "    RobustScaler\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    make_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle metric utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script exists to reduce code duplication across metrics.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def treat_as_participant_error(\n",
    "    error_message: str, solution: Union[pd.DataFrame, np.ndarray]\n",
    ") -> bool:\n",
    "    \"\"\"Many metrics can raise more errors than can be handled manually. This function attempts\n",
    "    to identify errors that can be treated as ParticipantVisibleError without leaking any competition data.\n",
    "\n",
    "    If the solution is purely numeric, and there are no numbers in the error message,\n",
    "    then the error message is sufficiently unlikely to leak usable data and can be shown to participants.\n",
    "\n",
    "    We expect this filter to reject many safe messages. It's intended only to reduce the number of errors we need to manage manually.\n",
    "    \"\"\"\n",
    "    # This check treats bools as numeric\n",
    "    if isinstance(solution, pd.DataFrame):\n",
    "        solution_is_all_numeric = all(\n",
    "            [pandas.api.types.is_numeric_dtype(x) for x in solution.dtypes.values]\n",
    "        )\n",
    "        solution_has_bools = any(\n",
    "            [pandas.api.types.is_bool_dtype(x) for x in solution.dtypes.values]\n",
    "        )\n",
    "    elif isinstance(solution, np.ndarray):\n",
    "        solution_is_all_numeric = pandas.api.types.is_numeric_dtype(solution)\n",
    "        solution_has_bools = pandas.api.types.is_bool_dtype(solution)\n",
    "\n",
    "    if not solution_is_all_numeric:\n",
    "        return False\n",
    "\n",
    "    for char in error_message:\n",
    "        if char.isnumeric():\n",
    "            return False\n",
    "    if solution_has_bools:\n",
    "        if \"true\" in error_message.lower() or \"false\" in error_message.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def safe_call_score(metric_function, solution, submission, **metric_func_kwargs):\n",
    "    \"\"\"\n",
    "    Call score. If that raises an error and that already been specifically handled, just raise it.\n",
    "    Otherwise make a conservative attempt to identify potential participant visible errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score_result = metric_function(solution, submission, **metric_func_kwargs)\n",
    "    except Exception as err:\n",
    "        error_message = str(err)\n",
    "        if err.__class__.__name__ == \"ParticipantVisibleError\":\n",
    "            raise ParticipantVisibleError(error_message)\n",
    "        elif err.__class__.__name__ == \"HostVisibleError\":\n",
    "            raise HostVisibleError(error_message)\n",
    "        else:\n",
    "            if treat_as_participant_error(error_message, solution):\n",
    "                raise ParticipantVisibleError(error_message)\n",
    "            else:\n",
    "                raise err\n",
    "    # Explicit float conversion prevents issues with numbers stored as np.float64 scalars\n",
    "    return float(score_result)\n",
    "\n",
    "\n",
    "def verify_valid_probabilities(df: pd.DataFrame, df_name: str):\n",
    "    \"\"\"Verify that the dataframe contains valid probabilities.\n",
    "\n",
    "    The dataframe must be limited to the target columns; do not pass in any ID columns.\n",
    "    \"\"\"\n",
    "    if not pandas.api.types.is_numeric_dtype(df.values):\n",
    "        raise ParticipantVisibleError(f\"All target values in {df_name} must be numeric\")\n",
    "\n",
    "    if df.min().min() < 0:\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"All target values in {df_name} must be at least zero\"\n",
    "        )\n",
    "\n",
    "    if df.max().max() > 1:\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"All target values in {df_name} must be no greater than one\"\n",
    "        )\n",
    "\n",
    "    if not np.allclose(df.sum(axis=1), 1):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Target values in {df_name} do not add to one within all rows\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    weights_column_name: Optional[str] = None,\n",
    "    adjusted: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Wrapper for https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
    "    Compute the balanced accuracy.\n",
    "\n",
    "    The balanced accuracy in binary and multiclass classification problems to\n",
    "    deal with imbalanced datasets. It is defined as the average of recall\n",
    "    obtained on each class.\n",
    "\n",
    "    The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : 1d DataFrame\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "    submission : 1d DataFrame\n",
    "    Estimated targets as returned by a classifier.\n",
    "\n",
    "    weights_column_name: optional str, the name of the sample weights column in the solution file.\n",
    "\n",
    "    adjusted : bool, default=False\n",
    "    When true, the result is adjusted for chance, so that random\n",
    "    performance would score 0, while keeping perfect performance at a score\n",
    "    of 1.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
    "    The balanced accuracy and its posterior distribution.\n",
    "    Proceedings of the 20th International Conference on Pattern\n",
    "    Recognition, 3121-24.\n",
    "    .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
    "    `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
    "    Algorithms, Worked Examples, and Case Studies\n",
    "    <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> y_true = [0, 1, 0, 0, 1, 0]\n",
    "    >>> y_true = pd.DataFrame(y_true)\n",
    "    >>> y_true[\"id\"] = range(len(y_true))\n",
    "    >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
    "    >>> y_pred = pd.DataFrame(y_pred)\n",
    "    >>> y_pred[\"id\"] = range(len(y_pred))\n",
    "    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n",
    "    0.625\n",
    "    \"\"\"\n",
    "    # Skip sorting and equality checks for the row_id_column since that should already be handled\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    sample_weight = None\n",
    "    if weights_column_name:\n",
    "        if weights_column_name not in solution.columns:\n",
    "            raise ValueError(\n",
    "                f\"The solution weights column {weights_column_name} is not found\"\n",
    "            )\n",
    "        sample_weight = solution.pop(weights_column_name).values\n",
    "        if not pandas.api.types.is_numeric_dtype(sample_weight):\n",
    "            raise ParticipantVisibleError(\"The solution weights are not numeric\")\n",
    "\n",
    "    if len(submission.columns) > 1:\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"The submission can only include one column of predictions. Found {len(submission.columns)}\"\n",
    "        )\n",
    "\n",
    "    solution = solution.values\n",
    "    submission = submission.values\n",
    "\n",
    "    score_result = safe_call_score(\n",
    "        balanced_accuracy_score,\n",
    "        solution,\n",
    "        submission,\n",
    "        sample_weight=sample_weight,\n",
    "        adjusted=adjusted,\n",
    "    )\n",
    "\n",
    "    return score_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATASETS = os.path.expanduser(\n",
    "    \"~/Projects/GoIT/MACHINE-LEARNING-NEO/datasets/final/data\"\n",
    ")\n",
    "PATH_TO_RESULTS = os.path.expanduser(\n",
    "    \"~/Projects/GoIT/MACHINE-LEARNING-NEO/datasets/final/results\"\n",
    ")\n",
    "os.makedirs(PATH_TO_RESULTS, exist_ok=True)\n",
    "print(PATH_TO_DATASETS)\n",
    "print(PATH_TO_RESULTS)\n",
    "\n",
    "train_file = os.path.join(PATH_TO_DATASETS, \"final_proj_data.csv\")\n",
    "test_file = os.path.join(PATH_TO_DATASETS, \"final_proj_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# \"customer churn\"\n",
    "TARGET = \"y\"\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "display(train_df.head())\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö —ñ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "print(\"\\nData types in training set:\")\n",
    "print(train_df.dtypes.value_counts())\n",
    "\n",
    "# Analyze missing values\n",
    "missing_summary = train_df.isnull().sum()\n",
    "missing_percent = (missing_summary / len(train_df)) * 100\n",
    "missing_data = pd.DataFrame({'Missing Count': missing_summary, \n",
    "                            'Missing Percent': missing_percent})\n",
    "missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing Percent', ascending=False)\n",
    "\n",
    "print(\"\\nFeatures with missing values:\")\n",
    "print(missing_data)\n",
    "\n",
    "sns.heatmap(train_df.isna(), cbar=False, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –í–∏–¥–∞–ª—è—î–º–æ –ø—É—Å—Ç—ñ –∫–æ–ª–æ–Ω–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = missing_summary[missing_summary == len(train_df)].index.tolist()\n",
    "if columns_to_drop:\n",
    "    print(f\"Removing {len(columns_to_drop)} completely empty columns: {columns_to_drop}\")\n",
    "    train_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    test_df.drop(columns=columns_to_drop, errors=\"ignore\", inplace=True)\n",
    "\n",
    "sns.heatmap(train_df.isna(), cbar=False, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í–∏–¥–∞–ª—è—î–º–æ –ø—É—Å—Ç—ñ —Ä—è–¥–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "empty_rows_train = train_df.isna().all(axis=1).sum()\n",
    "print(f\"Number of completely empty rows in training data: {empty_rows_train} out of {len(train_df)} ({empty_rows_train/len(train_df)*100:.2f}%)\")\n",
    "\n",
    "empty_rows_test = test_df.isna().all(axis=1).sum()\n",
    "print(f\"Number of completely empty rows in test data: {empty_rows_test} out of {len(test_df)} ({empty_rows_test/len(test_df)*100:.2f}%)\")\n",
    "\n",
    "train_df_cleaned = train_df.dropna(how='all')\n",
    "test_df_cleaned = test_df.dropna(how='all')\n",
    "\n",
    "print(f\"Removed {len(train_df) - len(train_df_cleaned)} rows from training data\")\n",
    "print(f\"Removed {len(test_df) - len(test_df_cleaned)} rows from test data\")\n",
    "\n",
    "train_df = train_df_cleaned\n",
    "test_df = test_df_cleaned\n",
    "\n",
    "print(f\"New training data shape: {train_df.shape}\")\n",
    "print(f\"New test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–æ–∑–ø–æ–¥—ñ–ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df[TARGET], kde=True)\n",
    "plt.title(\"Distribution  Target Variable (customer churn)\")\n",
    "plt.xlabel(\"customer churn\")\n",
    "target_counts = train_df[TARGET].value_counts()\n",
    "ax = sns.countplot(x=TARGET, data=train_df)\n",
    "for i, count in enumerate(target_counts):\n",
    "    percentage = 100 * count / len(train_df)\n",
    "    ax.text(i, count + 5, f\"{percentage:.1f}%\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "class_imbalance = train_df[TARGET].value_counts().max() / train_df[TARGET].value_counts().min()\n",
    "print(f\"\\nClass imbalance ratio (majority:minority): {class_imbalance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[TARGET], errors=\"ignore\")\n",
    "y = train_df[TARGET]\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "num_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features: {len(num_features)}\")\n",
    "print(f\"Categorical features: {len(cat_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "if (cat_features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    gs = gridspec.GridSpec(len(cat_features) // 2 + 1, 2)\n",
    "\n",
    "    for i, feature in enumerate(cat_features):\n",
    "        ax = plt.subplot(gs[i // 2, i % 2])\n",
    "\n",
    "        unique_count = train_df[feature].dropna().nunique()\n",
    "        value_counts = train_df[feature].value_counts(dropna=False).head(10)\n",
    "        value_counts.plot(kind='bar', ax=ax)\n",
    "        ax.set_title(f\"{feature} (unique: {unique_count})\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Unique values in categorical columns:\")\n",
    "for col in train_df.select_dtypes(include=[\"object\"]).columns:\n",
    "    print(f\"{col}: {train_df[col].nunique()} unique values\")\n",
    "\n",
    "    unique_values = train_df[col].unique()\n",
    "    if len(unique_values) <= 10:\n",
    "        print(f\"    Values: {unique_values}\")\n",
    "    else:\n",
    "        print(f\"    Sample values: {unique_values[:5]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "gs = gridspec.GridSpec(min(10, len(num_features)) // 2 + 1, 2)\n",
    "\n",
    "for i, feature in enumerate(num_features[:10]):  # Limit to first 10 features for visibility\n",
    "    ax = plt.subplot(gs[i // 2, i % 2])\n",
    "    sns.histplot(train_df[feature].dropna(), kde=True, ax=ax)\n",
    "    ax.set_title(f\"Distribution of {feature}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "numeric_data = train_df[num_features].copy()\n",
    "corr_matrix = numeric_data.corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=False, center=0)\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "high_corr_threshold = 0.8\n",
    "high_corr_features = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "            feat_pair = (corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j])\n",
    "            high_corr_features.append(feat_pair)\n",
    "            high_corr_features.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "if high_corr_features:\n",
    "    top_n = min(3, len(high_corr_features))\n",
    "    fig, axes = plt.subplots(1, top_n, figsize=(15, 5))\n",
    "    if top_n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(top_n):\n",
    "        feat1, feat2, corr = high_corr_features[i]\n",
    "        sns.scatterplot(x=train_df[feat1], y=train_df[feat2], hue=train_df[TARGET], ax=axes[i])\n",
    "        axes[i].set_title(f\"{feat1} vs {feat2}\\nCorr: {corr:.2f}\")\n",
    "        axes[i].set_xlabel(feat1)\n",
    "        axes[i].set_ylabel(feat2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    for feat1, feat2, corr in high_corr_features:\n",
    "        print(f\"{feat1} and {feat2}: {corr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "if high_corr_features:\n",
    "    print(f\"\\nRemoving {len(high_corr_features)} highly correlated features:\")\n",
    "    for feat1, feat2, corr in high_corr_features:\n",
    "        # if corr == 1:\n",
    "            # Check if the feature exists before trying to drop it\n",
    "        if feat2 in train_df.columns:\n",
    "            print(f\"Removing {feat2} due to perfect correlation with {feat1}\")\n",
    "            train_df.drop(columns=feat2, errors=\"ignore\", inplace=True)\n",
    "            test_df.drop(columns=feat2, errors=\"ignore\", inplace=True)\n",
    "else:\n",
    "    print(\"\\nNo highly correlated features to remove.\")\n",
    "\n",
    "   \n",
    "print(f\"New training data shape: {train_df.shape}\")\n",
    "print(f\"New test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "X_sample = X.copy()\n",
    "\n",
    "for col in num_features:\n",
    "    X_sample[col] = X_sample[col].fillna(X_sample[col].median())\n",
    "for col in cat_features:\n",
    "    X_sample[col] = X_sample[col].fillna(X_sample[col].mode()[0] if not X_sample[col].mode().empty else \"Unknown\")\n",
    "\n",
    "X_encoded = pd.get_dummies(X_sample, columns=cat_features, drop_first=True)\n",
    "\n",
    "print(\"\\nRunning preliminary feature importance analysis...\")\n",
    "rf_prelim = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_prelim.fit(X_encoded, y)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Importance': rf_prelim.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Features by Importance (Preliminary - Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X_encoded)\n",
    "    \n",
    "pca = PCA(n_components=min(20, X_scaled.shape[1]))\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "loadings = pca.components_.T\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "pca_importance = np.zeros(loadings.shape[0])\n",
    "for i in range(len(explained_variance_ratio)):\n",
    "    pca_importance += np.abs(loadings[:, i]) * explained_variance_ratio[i]\n",
    "\n",
    "pca_importance = pca_importance / np.sum(pca_importance)\n",
    "\n",
    "pca_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'PCA_Importance': pca_importance\n",
    "}).sort_values('PCA_Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='PCA_Importance', y='Feature', data=pca_feature_importance.head(20))\n",
    "plt.title('Top 20 Features by Importance (PCA-based Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "rf_top_features = feature_importance.head(20)['Feature'].tolist()\n",
    "pca_top_features = pca_feature_importance.head(20)['Feature'].tolist()\n",
    "\n",
    "common_features = set(rf_top_features).intersection(set(pca_top_features))\n",
    "\n",
    "print(f\"\\nNumber of common features in top 20 between RF and PCA: {len(common_features)}\")\n",
    "print(\"Common important features:\")\n",
    "for feature in common_features:\n",
    "    rf_rank = rf_top_features.index(feature) + 1\n",
    "    pca_rank = pca_top_features.index(feature) + 1\n",
    "    print(f\"  - {feature} (RF rank: {rf_rank}, PCA rank: {pca_rank})\")\n",
    "\n",
    "feature_importance['RF_Importance_Normalized'] = feature_importance['Importance'] / feature_importance['Importance'].max()\n",
    "pca_feature_importance['PCA_Importance_Normalized'] = pca_feature_importance['PCA_Importance'] / pca_feature_importance['PCA_Importance'].max()\n",
    "\n",
    "combined_importance = pd.merge(\n",
    "    feature_importance[['Feature', 'RF_Importance_Normalized']], \n",
    "    pca_feature_importance[['Feature', 'PCA_Importance_Normalized']], \n",
    "    on='Feature'\n",
    ")\n",
    "\n",
    "combined_importance['Combined_Score'] = (combined_importance['RF_Importance_Normalized'] + \n",
    "                                        combined_importance['PCA_Importance_Normalized']) / 2\n",
    "\n",
    "combined_importance = combined_importance.sort_values('Combined_Score', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Combined_Score', y='Feature', data=combined_importance.head(20))\n",
    "plt.title('Top 20 Features by Combined Importance (RF + PCA)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipeline setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"New training data shape: {train_df.shape}\")\n",
    "print(f\"New test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[TARGET], errors=\"ignore\")\n",
    "y = train_df[TARGET]\n",
    "\n",
    "num_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features: {len(num_features)}\")\n",
    "print(f\"Categorical features: {len(cat_features)}\")\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in [\"int64\", \"float64\"]:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            X[col] = imputer.fit_transform(X[col].values.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "        X[col].fillna(\"Unknown\", inplace=True)\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "            (\"imputer\", KNNImputer(n_neighbors=7)),\n",
    "            (\"scaler\", RobustScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False,\n",
    "                                    max_categories=20)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_features),\n",
    "        (\"cat\", categorical_transformer, cat_features),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# logreg_pipeline = ImbPipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\"feature_selection\", SelectKBest(f_classif, k=30)),\n",
    "#         (\"sampling\", SMOTE(random_state=42, sampling_strategy=0.5)),\n",
    "#         (\n",
    "#             \"classifier\",\n",
    "#             LogisticRegression(\n",
    "#                 max_iter=2000,\n",
    "#                 class_weight=\"balanced\",\n",
    "#                 solver=\"liblinear\",\n",
    "#                 C=0.5,\n",
    "#                 penalty=\"l1\"\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# rf_pipeline = ImbPipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\"sampling\", SMOTE(random_state=42, sampling_strategy=0.4)),\n",
    "#         (\n",
    "#             \"classifier\",\n",
    "#             RandomForestClassifier(\n",
    "#                 class_weight={0: 1, 1: 6},\n",
    "#                 random_state=42,\n",
    "#                 n_estimators=200,\n",
    "#                 max_depth=12,\n",
    "#                 min_samples_split=3,\n",
    "#                 min_samples_leaf=2,\n",
    "#                 bootstrap=True,\n",
    "#                 max_features='sqrt'\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# gb_pipeline = ImbPipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\"sampling\", SMOTE(random_state=42, sampling_strategy=0.4)),\n",
    "#         (\n",
    "#             \"classifier\",\n",
    "#             GradientBoostingClassifier(\n",
    "#                 random_state=42,\n",
    "#                 n_estimators=200,\n",
    "#                 learning_rate=0.05,\n",
    "#                 max_depth=4,\n",
    "#                 subsample=0.8,\n",
    "#                 min_samples_split=5,\n",
    "#                 min_samples_leaf=5\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# voting_pipeline = ImbPipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\"sampling\", SMOTE(random_state=42, sampling_strategy=0.4)),\n",
    "#         (\n",
    "#             \"classifier\",\n",
    "#             VotingClassifier(\n",
    "#                 estimators=[\n",
    "#                     ('lr', LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"liblinear\", C=0.5, penalty=\"l1\")),\n",
    "#                     ('rf', RandomForestClassifier(class_weight={0: 1, 1: 6}, random_state=42, n_estimators=200, max_depth=12)),\n",
    "#                     ('gb', GradientBoostingClassifier(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=4))\n",
    "#                 ],\n",
    "#                 voting='soft'\n",
    "#             )\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# xgb_pipeline = ImbPipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\"sampling\", SMOTE(random_state=42, sampling_strategy=0.4)),\n",
    "#         (\n",
    "#             \"classifier\",\n",
    "#             XGBClassifier(\n",
    "#                 n_estimators=200,\n",
    "#                 learning_rate=0.05,\n",
    "#                 max_depth=4,\n",
    "#                 use_label_encoder=False,\n",
    "#                 eval_metric=\"logloss\",\n",
    "#                 scale_pos_weight=5,\n",
    "#                 random_state=42,\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "cat_indices = [i for i, col in enumerate(X.columns) if col in cat_features]\n",
    "\n",
    "xgb_pipeline = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"sampling\",\n",
    "            SMOTENC(\n",
    "                categorical_features=cat_indices, random_state=42, sampling_strategy=0.4\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.03,\n",
    "                max_depth=5,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.8,\n",
    "                min_child_weight=3,\n",
    "                gamma=0.5,\n",
    "                scale_pos_weight=6.5,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__max_depth\": [4, 5, 6],\n",
    "    \"classifier__learning_rate\": [0.02, 0.03, 0.05],\n",
    "    \"classifier__scale_pos_weight\": [6, 6.5, 7],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"üîç –ü–æ—à—É–∫ –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ —á–µ—Ä–µ–∑ GridSearchCV...\")\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"\\n‚úÖ Best parameters found:\")\n",
    "print(grid.best_params_)\n",
    "print(f\"üèÜ Best balanced accuracy: {grid.best_score_:.4f}\")\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# model_pipelines = {\n",
    "#     # \"Logistic Regression\": logreg_pipeline,\n",
    "#     # \"Random Forest\": rf_pipeline,\n",
    "#     # \"Gradient Boosting\": gb_pipeline,\n",
    "#     # \"Voting Ensemble\": voting_pipeline,\n",
    "#     \"XGBoostClassifier\": xgb_pipeline,\n",
    "# }\n",
    "\n",
    "# results = {}\n",
    "\n",
    "# for name, pipeline in model_pipelines.items():\n",
    "#     print(f\"\\nEvaluating {name}...\")\n",
    "#     scores = []\n",
    "    \n",
    "#     for train_idx, val_idx in cv.split(X, y):\n",
    "#         X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "#         y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "#         pipeline.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "#         y_pred_fold = pipeline.predict(X_val_fold)\n",
    "        \n",
    "#         solution_df = pd.DataFrame({\n",
    "#             'row_id': range(len(y_val_fold)),  \n",
    "#             'y': y_val_fold.values\n",
    "#         })\n",
    "        \n",
    "#         submission_df = pd.DataFrame({\n",
    "#             'row_id': range(len(y_pred_fold)),  \n",
    "#             'prediction': y_pred_fold\n",
    "#         })\n",
    "        \n",
    "#         fold_score = score(\n",
    "#             solution=solution_df.copy(),\n",
    "#             submission=submission_df.copy(),\n",
    "#             row_id_column_name='row_id'\n",
    "#         )\n",
    "        \n",
    "#         scores.append(fold_score)\n",
    "    \n",
    "#     results[name] = {\n",
    "#         \"scores\": np.array(scores),\n",
    "#         \"mean\": np.mean(scores),\n",
    "#         \"std\": np.std(scores)\n",
    "#     }\n",
    "    \n",
    "#     print(f\"{name} CV Scores: {scores}\")\n",
    "#     print(f\"{name} Mean Balanced Accuracy: {results[name]['mean']:.4f} ¬± {results[name]['std']:.4f}\")\n",
    "\n",
    "# best_model_name = max(results, key=lambda k: results[k]['mean'])\n",
    "# best_model = model_pipelines[best_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBest model: {best_model_name} with balanced accuracy: {results[best_model_name]['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTraining {best_model} on full training dataset...\")\n",
    "best_model.fit(X, y)\n",
    "\n",
    "if \"row_id\" not in test_df.columns:\n",
    "    test_df[\"row_id\"] = range(len(test_df))\n",
    "\n",
    "print(\"Generating predictions on test data...\")\n",
    "test_predictions = best_model.predict(test_df.drop(columns=[\"row_id\"], errors=\"ignore\"))\n",
    "\n",
    "submission_df = pd.DataFrame(\n",
    "    {\"index\": range(len(test_predictions)), TARGET: test_predictions}\n",
    ")\n",
    "\n",
    "submission_path = os.path.join(PATH_TO_RESULTS, \"submission.csv\")\n",
    "submission_df.to_csv(submission_path, index=False, header=False)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "unique_predictions, counts = np.unique(test_predictions, return_counts=True)\n",
    "print(\"\\nPrediction distribution on test data:\")\n",
    "for value, count in zip(unique_predictions, counts):\n",
    "    print(f\"  Class {value}: {count} ({count/len(test_predictions)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nSubmission file contains {len(submission_df)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# if 'row_id' not in test_df.columns:\n",
    "#     test_df['row_id'] = range(len(test_df))\n",
    "\n",
    "# print(f\"\\nTraining {best_model_name} on full training dataset...\")\n",
    "# best_model.fit(X, y)\n",
    "\n",
    "# print(\"Generating predictions on test data...\")\n",
    "# test_predictions = best_model.predict(test_df.drop(columns=['row_id'], errors='ignore'))\n",
    "\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'index': range(len(test_predictions)),\n",
    "#     'y': test_predictions\n",
    "# })\n",
    "\n",
    "# submission_path = os.path.join(PATH_TO_RESULTS, \"submission.csv\")\n",
    "# submission_df.to_csv(submission_path, index=False, header=False)\n",
    "# print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "# solution_df = pd.DataFrame({\n",
    "#     'row_id': range(len(test_df)),\n",
    "#     'y': np.zeros(len(test_df))\n",
    "# })\n",
    "\n",
    "# prediction_df = pd.DataFrame({\n",
    "#     'row_id': range(len(test_predictions)),\n",
    "#     'prediction': test_predictions\n",
    "# })\n",
    "\n",
    "# unique_predictions, counts = np.unique(test_predictions, return_counts=True)\n",
    "# print(\"\\nPrediction distribution on test data:\")\n",
    "# for value, count in zip(unique_predictions, counts):\n",
    "#     print(f\"  Class {value}: {count} ({count/len(test_predictions)*100:.2f}%)\")\n",
    "\n",
    "# print(f\"\\nSubmission file contains {len(submission_df)} predictions\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
